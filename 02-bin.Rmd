# The Binomial Model {#bin}

We now start looking at actual Bayesian inference for a variety of data types and models, starting from the simplest case of binary outcomes. 

Consider the following motivating example. A survey conducted in 2020 asked 2000 Spaniards whether they were happy or not. 920 of the respondents said they were indeed happy. Such a situation could be modeled using a *Binomial* model if we were to believe the following two assumptions:

 - each respondent has a same unknown probability $\theta$ of replying yes;
 
 - each respondent is independent of all others.
 
Under these assumptions, let $Y$ be the random variable denoting the number of respondents that said they were happy. Then 
\[
P(Y=y|\theta) = \binom{n}{y}\theta^{y}(1-\theta)^{n-y}.
\]
by recalling the form of the Binomial probabilities. In general we say that $Y$ follows a Binomial distribution with parameters $n$ and $\theta$ if its pdf can be written as above. Then we also have that $E(Y)=n\theta$ and $V(Y)=n\theta(1-\theta)$.

Notice that we explicitly write $P(Y=y|\theta)$, meaning that these probabilities are conditional on an unknown parameter $\theta$ denoting the probability that an individual is happy. The number $n$ is not considered random - the number of people interviewed. This is indeed usually fixed by design choices.

Assume that we fixed $\theta = 0.5$, an individual is equally likely to be happy/unhappy. Then the probability of observing 920 happy individuals out of 2000 is:
\[
P(Y=920|\theta=0.5)=\binom{2000}{920}0.5^{920}(1-0.5)^{1080}.
\]
Using R this number can be computed as
```{r}
dbinom(920, size = 2000, prob = 0.5)
```

The overall distribution of a Binomial with parameters $n=2000$ and $\theta = 0.5$ is reported in Figure \@ref(fig:dbinom).

```{r dbinom, fig.align='center',out.width="50%", fig.cap = "Probability density function of a Binomial with parameters 2000 and 0.5"}
x <- 0:2000
qplot(x, ymax = dbinom(x, size = 2000, prob = 0.5), ymin = 0, 
      geom = "linerange",  xlab = "number of successes", ylab = "probability") +
  theme_bw()
```

Our aim in this chapter is to develop methods to answer the following question: given a sample $y_1,\dots,y_N$ of independent and identically distributed binary outcomes (just as in our happiness survey) and some prior distribution $p(\theta)$ for the parameter of success $\theta$, what are our posterior beliefs $p(\theta|y_1,\dots,y_n)$ and how can we summarize them?

## Inference Using a Uniform Prior

Let's consider again our happiness survey. We collected a sample $y_1,\dots,y_{2000}$ of binary outcomes happy/unhappy (1/0) of which 920 replied they were happy. The likelihood of this data is therefore
\[
p(y|\theta)=\binom{n}{\sum_{i=1}^ny_i}\theta^{\sum_{i=1}^ny_i}(1-\theta)^{n-\sum_{i=1}^ny_i}
\]
In order to complete our model definition we also need to define a prior distribution for the parameter $\theta$, which takes values between zero and one.

Let's start choosing a uniform prior distribution between zero and one, that is:
\[
p(\theta)=\left\{
\begin{array}{ll}
1, & 0\leq \theta \leq 1\\
0, & \mbox{otherwise}
\end{array}
\right.
\]
and reported in Figure \@ref(fig:unipdf).

```{r unipdf, fig.align='center',fig.cap="Probability density function of the Uniform between zero and one", echo = F, warning = F, message= FALSE, out.width="50%"}
library(ggplot2)
x <- c(seq(-1,0,0.01),seq(0,1,0.01),seq(1,2,0.01))
y <- c(rep(0,length(seq(-1,0,0.01))),rep(1,length(seq(0,1,0.01))),rep(0,length(seq(1,2,0.01))))
p1 <- ggplot(data.frame(x=x,y=y),aes(x,y)) + geom_line(lwd=1.3,col="red") + theme_bw() + ylab("f(x)")
x <- c(seq(-1,0,0.01),seq(0,1,0.01),seq(1,2,0.01))
y <- c(rep(0,length(seq(-1,0,0.01))),seq(0,1,0.01),rep(1,length(seq(1,2,0.01))))
p1
```

The assumption of a Uniform distribution implies that the same probability is given to any subinterval of $[0,1]$ of the same length. This is the simplest prior distribution we can choose.

Given these prior and data-generating process our posterior is:
\begin{eqnarray*}
p(\theta|y_1,\dots,y_{2000}) &=&\frac{p(y_1,\dots,y_{2000}|\theta)p(\theta)}{p(y_1,\dots,y_{2000})}\\
&=&\frac{p(y_1,\dots,y_{2000}|\theta)\cdot 1}{p(y_1,\dots,y_{2000})}\\
&\propto&p(y_1,\dots,y_{2000}|\theta)\\
&=& \binom{n}{\sum_{i=1}^ny_i}\theta^{\sum_{i=1}^ny_i}(1-\theta)^{n-\sum_{i=1}^ny_i}\\
&\propto& \theta^{\sum_{i=1}^ny_i}(1-\theta)^{n-\sum_{i=1}^ny_i}\\
&=& \theta^{920}(1-\theta)^{1080}
\end{eqnarray*}

The expression $\theta^{920}(1-\theta)^{1080}$ is proportional to the posterior distribution $p(\theta|y_1,\dots,y_{2000})$, meaning that it does not integrate to one. Using results from calculus it can be indeed shown that
\[
\int_{0}^{1}\theta^{920}(1-\theta)^{1080}d\theta=\frac{\Gamma(921)\Gamma(1081)}{\Gamma(921 + 1081)}
\] 
where $\Gamma(\cdot)$ is the so-called Gamma function (its value for any $x>0$ can be computed in `R` using `gamma(x)`).

How is the above integral result useful to derive the form of the posterior? Recall that the posterior is
\[
p(\theta|y_1,\dots,y_{2000}) = \theta^{920}(1-\theta)^{1080}\frac{1}{p(y_1,\dots,y_{2000})},
\]
and it must be such that 
\[
\int_0^1p(\theta|y_1,\dots,y_{2000})d\theta = 1. 
\]
Using everything that we have learned so far we can then deduce that 
\begin{eqnarray*}
1 & = & \int_0^1p(\theta|y_1,\dots,y_{2000})d\theta \\
& = &  \int_0^1 \theta^{920}(1-\theta)^{1080}\frac{1}{p(y_1,\dots,y_{2000})}d\theta\\
& = &\frac{1}{p(y_1,\dots,y_{2000})} \int_0^1 \theta^{920}(1-\theta)^{1080}d\theta\\
&= & \frac{1}{p(y_1,\dots,y_{2000})} \frac{\Gamma(921)\Gamma(1081)}{\Gamma(921 + 1081)}
\end{eqnarray*}
This implies that what we called the marginal likelihood
\[
p(y_1,\dots,y_{2000})=\frac{\Gamma(921)\Gamma(1081)}{\Gamma(921 + 1081)}
\]
and therefore the posterior is
\[
p(\theta|y_1,\dots,y_{2000}) = \frac{\Gamma(921 + 1081)}{\Gamma(921)\Gamma(1081)}\theta^{920}(1-\theta)^{1080}.
\]
Although you probably have never seen this expression, the above is the density of the so-called *Beta* distribution, which will be formally introduced next.

Before this, let's consider Figure \@ref(fig:binpostuni). The prior distribution is reported by the blue line and is the flat uniform. Given that we observed 920 individuals who are happy our posterior distribution now reflects the information in the sample and it peaks around the sample proportion 920/2000=0.46. Furthermore, the variance has decreased and the density is concentrated around the sample proportion.

```{r binpostuni, fig.align='center',fig.cap="Prior and posterior distribution for the happiness survey", out.width="50%"}
ggplot(data.frame(x=c(0,1)), aes(x)) +
  stat_function(fun= function(x) dbeta(x, 921, 1081),aes(colour="Posterior"))+
   stat_function(fun= function(x) dunif(x),aes(colour="Prior"))+
  theme_bw() + ylab("p(theta)") + xlab("theta")
```

## The Beta Distribution

A random variable $\theta$ is said to follow the Beta distributions with parameters $a$ and $b$ if its pdf is
\[
p(\theta)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}
\]
for $\theta\in[0,1]$. The pdf for various choices of $a$ and $b$ is reported in Figure \@ref(fig:dbeta). Importantly, we can see that the Uniform distribution is a special case of the Beta distribution when parameters are fixed to $a=1$ and $b=1$. Indeed,
\[
p(\theta)=\frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}\theta^{0}(1-\theta)^{0}=\frac{1}{1\cdot 1} 1\cdot 1 = 1,
\]
since $\Gamma(x)=x!$ if $x>1$ and integer, and $\Gamma(1)=1$.
```{r dbeta, out.width="50%", fig.align='center', fig.cap="Density of the Beta distribution for various choices of parameters."}
ggplot(data.frame(x=seq(0,1,0.01)), aes(x)) +
  stat_function(fun= function(x) dbeta(x,0.5,0.5),aes(colour="a = 0.5, b = 0.5"))+
     stat_function(fun= function(x) dbeta(x,1,1),aes(colour="a = 1, b = 1"))+
    stat_function(fun= function(x) dbeta(x,1,3),aes(colour="a = 1, b = 3"))+
    stat_function(fun= function(x) dbeta(x,5,2),aes(colour="a = 5, b = 2"))+
  theme_bw() + ylab("p(theta)") + xlab("theta") + labs(colour = "Parameters")
```

If $\theta$ follows a Beta random variable with parameters $a$ and $b$, one can prove that

 - $E(\theta)=\frac{a}{a+b}$
 
 - $mode(\theta)=\frac{a-1}{(a-1)+(b-1)}$ if $a>1$ and $b>1$
 
 - $V(\theta)= \frac{ab}{(a+b)^2(a+b+1)}$

## Inference using a Beta Prior

Now we turn our attention to cases where the parameter $\theta$ is given a Beta prior distribution. First let's revisit the example of the prior uniform distribution.

### Uniform as Beta

We have a sample $y_1,\dots, y_n$ of independent and identically distributed (same probability of success) binary outcomes, so that $p(y_1,\dots,y_n|\theta)$ is Binomial with parameters $n$ and $\theta$. The prior distribution for $\theta$ is uniform, which is equivalent to a Beta distribution with parameters $a=1$ and $b=1$.

Then the posterior is such that 
\begin{equation}
p(\theta|y_1,\dots,y_n) \propto \theta^{\sum_{i=1}^ny_i}(1-\theta)^{n-\sum_{i=1}^ny_i} =\theta^y(1-\theta)^{n-y}
 (\#eq:betapost)
\end{equation}
where for simplicity we called $\sum_{i=1}^ny_i=y$, the number of successes. The above expression is a function of $\theta$ and we can spot that it has the elements of a Beta distribution: it is only missing the ratio $\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}$. However, all terms involving $\theta$ of the pdf of a Beta are in the expression. In particular, the above expression must be proportional to the density of a Beta with parameter $a= y+1$ and $b= n-y +1$. Notice in particular that the parameter $a$ of the posterior is the number of successes plus the parameter $a=1$ of the uniform prior and the parameter $b$ of the posterior is the number of failures plus the parameter $b=1$ of the prior.

Let's consider again the happiness survey data, where 920 individuals said that they were happy out of 2000. Using the properties of the Beta distribution, we then have that

 - $E(\theta|y_1,\dots,y_n)= \frac{921}{2002}=0.46004$;
 
 - $mode(\theta|y_1,\dots,y_n)=\frac{920}{2000}=0.46$;
 
 - $V(\theta|y_1,\dots,y_n)= 0.00012$

So we actually derived the posterior distribution, which is Beta with parameters $y+1$ and $n-y+1$, by looking at expression \@ref(eq:betapost) and recognizing that it must be proportional to the known Beta distribution. Furthermore, we also recognized the value of the parameters by comparing the expressions. This is a trick we will use multiple times which allows us to derive the posterior by only looking at 
\[
p(\theta|y)\propto p(y|\theta)p(\theta)
\]
instead of
\[
p(\theta|y) =  \frac{p(y|\theta)p(\theta)}{p(y)}
\]
which requires the computation of $p(y)$ often involving complex integration.

### A Generic Beta Prior

Let's now take a more general approach and let's suppose that $\theta$ is given a prior distribution $p(\theta)$ which is Beta with some parameters $a$ and $b$. What is the form of the posterior?

\begin{eqnarray*}
p(\theta|y_1,\dots,y_n)&\propto& p(y_1,\dots,y_n|\theta)p(\theta)\\
 &= & \binom{n}{y}\theta^y(1-\theta)^{n-y}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}\\
 &\propto& \theta^y(1-\theta)^{n-y}\theta^{a-1}(1-\theta)^{b-1}\\
 & = & \theta^{y+a-1}(1-\theta)^{n-y + b -1}
\end{eqnarray*}

Using the same trick as before, we notice that the above expression is proportional to the density of a Beta distribution with parameters $y+a$ and $n-y+b$. So we started with a prior Beta distribution and our posterior is again Beta. Such a property of a prior distribution $p(\theta)$ and a data-generating process $p(y|\theta)$ is usually referred to as *conjugacy*.

A class of prior distributions $\mathcal{P}$ for $\theta$ is said to be **conjugate** for a data-generating process $p(y|\theta)$ if
\[
p(\theta)\in\mathcal{P}\Rightarrow p(\theta|y)\in\mathcal{P}
\]

So the Beta distribution is the conjugate prior to the Binomial: a Beta prior combined to a Binomial likelihood gives a posterior distribution which is again Beta.

Using the properties of the Beta distribution we can then derive that:

 - $E(\theta|y_1,\dots,y_n)=\frac{a+y}{a+b+n}$
 
 - $mode(\theta|y_1,\dots,y_n)=\frac{a+y-1}{a+b+n-2}$
 
 - $V(\theta|y_1,\dots,y_n)=\frac{(a+y)(n-y+b)}{(n+a+b+1)(n+a+b)^2}$

Let's look into the posterior mean more carefully. We can rewrite it as
\begin{eqnarray*}
E(\theta|y_1,\dots,y_n)&=&\frac{a+y}{a+b+n}\\
 & = & \frac{a+b}{a+b+n}\frac{a}{a+b} + \frac{n}{a+b+n}\frac{y}{n}\\
 & = & \frac{a+b}{a+b+n}E(\theta) + \frac{n}{a+b+n}\bar{y}_n,
\end{eqnarray*}
where $\bar{y}_n$ is the sample mean.

So the posterior mean is a weighted average between the prior mean $E(\theta)$ and the sample mean $\bar{y}_n$. By looking at the weights we can also see that if $n >> a+b$ then $\frac{a+b}{a+b+n}\approx 0$ and $\frac{n}{a+b+n}\approx 1$ and therefore the posterior mean is equal to the sample mean. This means that if the sample size is very large, then our posterior beliefs are mostly driven by the data.

The parameters of the prior Beta distribution can be interpreted as follows:

 - $a+b$ is a prior sample size: the larger this number the more emphasis we want to put on the prior;
 
 - $a$ is the prior number of successes;
 
 - $b$ is the prior number of failures;
 
Figure \@ref(fig:effectbin) illustrates the effect of various parameter choices $a$ and $b$ for the prior distribution.
 
```{r effectbin, echo = F, fig.align='center',fig.cap="Posterior distribution for different prior distributions. Sample proportion - black vertical line. Top left: a = b = 5; Top right: a = b = 50; Bottom left: a = b = 500; Bottom right: a = b = 1500."}
library(gridExtra)
p1 <- ggplot(data.frame(x=c(0,1)), aes(x)) +
  stat_function(fun= function(x) dbeta(x, 925, 1085),aes(colour="Posterior"))+
   stat_function(fun= function(x) dbeta(x,5,5),aes(colour="Prior"))+
  theme_bw() + ylab("p(theta)") + xlab("theta") + ylim(0,50) + geom_vline(xintercept = 920/2000)
p2 <- ggplot(data.frame(x=c(0,1)), aes(x)) +
  stat_function(fun= function(x) dbeta(x, 970, 1130),aes(colour="Posterior"))+
   stat_function(fun= function(x) dbeta(x,50,50),aes(colour="Prior"))+
  theme_bw() + ylab("p(theta)") + xlab("theta")+ ylim(0,50) + geom_vline(xintercept = 920/2000)
p3 <- ggplot(data.frame(x=c(0,1)), aes(x)) +
  stat_function(fun= function(x) dbeta(x, 1420, 1580),aes(colour="Posterior"))+
   stat_function(fun= function(x) dbeta(x,500,500),aes(colour="Prior"))+
  theme_bw() + ylab("p(theta)") + xlab("theta")+ ylim(0,50) + geom_vline(xintercept = 920/2000)
p4 <- ggplot(data.frame(x=c(0,1)), aes(x)) +
  stat_function(fun= function(x) dbeta(x, 2420, 2580),aes(colour="Posterior"))+
   stat_function(fun= function(x) dbeta(x,1500,1500),aes(colour="Prior"))+
  theme_bw() + ylab("p(theta)") + xlab("theta")+ ylim(0,50) + geom_vline(xintercept = 920/2000)
grid.arrange(p1,p2,p3,p4,ncol=2,nrow=2)
```

## Predictive Distribution

An important feature of Bayesian inference is the existence of a *predictive distribution* which gives the probability of observing a specific new observation given that we have already observed a sample $y_1,\dots,y_n$, that is $p(\tilde{y}|y_1,\dots,y_n)$ where $\tilde{y}$ is a possible value of the random variable of interest. In the specific case of a binary outcome, we can compute $p(\tilde{y}=1|y_1,\dots,y_n)$ the probability of observing a success given that we observed the sample. 

Let's compute this probability.
\begin{eqnarray*}
p(\tilde{y}=1|y_1,\dots,y_n)&=& \int_{0}^1p(\tilde{y}=1,\theta|y_1,\dots,y_n)d\theta\\
&=& \int_{0}^1p(\tilde{y}=1|\theta,y_1,\dots,y_n)p(\theta|y_1,\dots,y_n)d\theta\\
&=&\int_{0}^1\theta p(\theta|y_1,\dots,y_n)d\theta\\
&=& E(\theta|y_1,\dots,y_n)
\end{eqnarray*}
So the predictive probability of a success is equal to the posterior mean in the case of binary outcomes. We will see that in other cases it is not as easy to derive this distribution.

